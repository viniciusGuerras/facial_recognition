{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viniciusGuerras/facial_recognition/blob/main/face_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importações e processamentos"
      ],
      "metadata": {
        "id": "xbEPamqaBToM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports e Downloads"
      ],
      "metadata": {
        "id": "7Rovy-f7Amrx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgCYmG5_lUs9",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install chromadb --quiet\n",
        "!pip install pytorch-metric-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf5gFhvKBv3-"
      },
      "outputs": [],
      "source": [
        "from pytorch_metric_learning import losses, miners\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from chromadb.utils import embedding_functions\n",
        "from torchvision import datasets, transforms\n",
        "from torch.nn import TripletMarginLoss\n",
        "from google.colab import drive\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import chromadb\n",
        "import random\n",
        "import torch\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWE339kDG_sf"
      },
      "outputs": [],
      "source": [
        "client = chromadb.Client()\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip \"/content/gdrive/MyDrive/post-processed.zip\" -d \"/content/data\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classe para ajudar com o \"path\" das imagens no dataset:"
      ],
      "metadata": {
        "id": "f805sHhPAqNU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTY7W5-C_BhJ"
      },
      "outputs": [],
      "source": [
        "class PathImageFolder(Dataset):\n",
        "    def __init__(self, image_folder_dataset, transform=None):\n",
        "        self.dataset = image_folder_dataset\n",
        "        self.samples = self.dataset.samples\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path, label = self.samples[index]\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, img_path, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carrega o Dataset:"
      ],
      "metadata": {
        "id": "G1Oqza-RAzNS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B97WGoTy9Ul-"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.RandomResizedCrop(160, scale=(0.9, 1.0)),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05),\n",
        "    transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "raw_dataset = []\n",
        "dataset = PathImageFolder(raw_dataset, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=128, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criação do modelo"
      ],
      "metadata": {
        "id": "eA1pO3PeBLCs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqZBm0YJd_WY"
      },
      "outputs": [],
      "source": [
        "class ResidualBottleneckBlock(nn.Module):\n",
        "  expansion = 2\n",
        "  def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "      super().__init__()\n",
        "      #layer 1\n",
        "      self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "      self.bn1 = nn.BatchNorm2d(planes)\n",
        "      #layer 2\n",
        "      self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "      self.bn2 = nn.BatchNorm2d(planes)\n",
        "      #layer 3\n",
        "      self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
        "      self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "\n",
        "      self.relu = nn.ReLU(inplace=True)\n",
        "      self.downsample = downsample\n",
        "\n",
        "  def forward(self, x):\n",
        "      identity = x\n",
        "      out = self.conv1(x)\n",
        "      out = self.bn1(out)\n",
        "      out = self.relu(out)\n",
        "\n",
        "      out = self.conv2(out)\n",
        "      out = self.bn2(out)\n",
        "      out = self.relu(out)\n",
        "\n",
        "      out = self.conv3(out)\n",
        "      out = self.bn3(out)\n",
        "      if self.downsample is not None:\n",
        "          identity = self.downsample(x)\n",
        "      out += identity\n",
        "      out = self.relu(out)\n",
        "      return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSc5DyU8ygNg"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class FaceNetResNet(nn.Module):\n",
        "    def __init__(self, block=ResidualBottleneckBlock, layers=[2,2,2,2], embedding_size=512):\n",
        "        super().__init__()\n",
        "        self.inplanes = 64\n",
        "        #first layer\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        #blocks layers\n",
        "        self.layer1 = self._make_layer(block, 64,  layers[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        #fully connected\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(512 * ResidualBottleneckBlock.expansion, embedding_size)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "      downsample = None\n",
        "      if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "        downsample = nn.Sequential(\n",
        "            nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "            nn.BatchNorm2d(planes * block.expansion),\n",
        "        )\n",
        "      layers = [block(self.inplanes, planes, stride, downsample)]\n",
        "      self.inplanes = planes * block.expansion\n",
        "      for _ in range(1, blocks):\n",
        "          layers.append(block(self.inplanes, planes))\n",
        "      return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return F.normalize(x, p=2, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVjsjIO0s5x5"
      },
      "source": [
        "## Loop de treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOqcDWIxs4ei"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = FaceNetResNet().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
        "loss_fn = losses.TripletMarginLoss(margin=0.8)\n",
        "miner = miners.MultiSimilarityMiner(epsilon=0.1)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "epochs = 250\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  skipped = 0\n",
        "  total_loss = 0\n",
        "  batch_count = 0\n",
        "\n",
        "  for batch_idx, batch in enumerate(dataloader):\n",
        "    imgs, paths, labels = batch\n",
        "    imgs = imgs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    embeddings = model(imgs)\n",
        "    hard_triplets = miner(embeddings, labels)\n",
        "\n",
        "    if len(hard_triplets) == 0:\n",
        "      skipped+= 1\n",
        "      continue\n",
        "\n",
        "    loss = loss_fn(embeddings, labels, hard_triplets)\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    batch_count += 1\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_idx % 50 == 0:\n",
        "      print(f\"Batch: {batch_idx}, Loss: {loss.item()}\")\n",
        "\n",
        "  avg_loss = total_loss / batch_count if batch_count > 0 else 0\n",
        "  scheduler.step(avg_loss)\n",
        "  print(f\"Epoch {epoch+1} complete. Average Loss: {avg_loss:.4f}. Skipped batches: {skipped}/{len(dataloader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "usa o modelo em todas as imagens e as salva na db"
      ],
      "metadata": {
        "id": "JmKh1fxgBl04"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_bkKFrS8vC_"
      },
      "outputs": [],
      "source": [
        "all_embeddings = []\n",
        "all_labels = []\n",
        "all_paths = []\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, paths, labels in dataloader:\n",
        "      imgs = imgs.to(device)\n",
        "      emb = model(imgs).cpu().numpy()\n",
        "      all_embeddings.extend(emb.tolist())\n",
        "      all_paths.extend(paths)\n",
        "      all_labels.extend(labels.tolist())\n",
        "\n",
        "collection = client.create_collection(\n",
        "    name=\"faces\",\n",
        "    embedding_function=None,\n",
        "    get_or_create=True\n",
        ")\n",
        "\n",
        "chromas_batch_size = 5000\n",
        "num_embeddings = len(all_embeddings)\n",
        "\n",
        "for i in range(0, num_embeddings, chromas_batch_size):\n",
        "    batch_embeddings = all_embeddings[i:i+chromas_batch_size]\n",
        "    batch_ids = [f\"img_{j}\" for j in range(i, i+len(batch_embeddings))]\n",
        "    batch_metadatas = [{\"label\": label} for label in all_labels[i:i+chromas_batch_size]]\n",
        "    batch_documents = [path for path in all_paths[i:i+chromas_batch_size]]\n",
        "\n",
        "    collection.add(\n",
        "        ids=batch_ids,\n",
        "        embeddings=batch_embeddings,\n",
        "        metadatas=batch_metadatas,\n",
        "        documents=batch_documents\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}