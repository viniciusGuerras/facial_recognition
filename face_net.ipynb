{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viniciusGuerras/facial_recognition/blob/main/face_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importações e processamentos"
      ],
      "metadata": {
        "id": "xbEPamqaBToM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports e Downloads"
      ],
      "metadata": {
        "id": "7Rovy-f7Amrx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgCYmG5_lUs9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01b41bd3-6fdb-4017-cd84-69e8dadb403b",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-metric-learning in /usr/local/lib/python3.12/dist-packages (2.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pytorch-metric-learning) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from pytorch-metric-learning) (1.6.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-metric-learning) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from pytorch-metric-learning) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->pytorch-metric-learning) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->pytorch-metric-learning) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->pytorch-metric-learning) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.6.0->pytorch-metric-learning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.6.0->pytorch-metric-learning) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install chromadb --quiet\n",
        "!pip install pytorch-metric-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf5gFhvKBv3-"
      },
      "outputs": [],
      "source": [
        "from pytorch_metric_learning import losses, miners\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from chromadb.utils import embedding_functions\n",
        "from torchvision import datasets, transforms\n",
        "from torch.nn import TripletMarginLoss\n",
        "from google.colab import drive\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import chromadb\n",
        "import random\n",
        "import torch\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWE339kDG_sf",
        "outputId": "a7a507d1-d379-4951-adbf-f34e6786b459",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Archive:  /content/gdrive/MyDrive/post-processed.zip\n",
            "replace /content/data/post-processed/AJ_Lamas/AJ_Lamas_0001_0000.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ],
      "source": [
        "client = chromadb.Client()\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip \"/content/gdrive/MyDrive/post-processed.zip\" -d \"/content/data\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classe para ajudar com o \"path\" das imagens no dataset:"
      ],
      "metadata": {
        "id": "f805sHhPAqNU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTY7W5-C_BhJ"
      },
      "outputs": [],
      "source": [
        "class PathImageFolder(Dataset):\n",
        "    def __init__(self, image_folder_dataset, transform=None):\n",
        "        self.dataset = image_folder_dataset\n",
        "        self.samples = self.dataset.samples\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path, label = self.samples[index]\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, img_path, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carrega o Dataset:"
      ],
      "metadata": {
        "id": "G1Oqza-RAzNS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B97WGoTy9Ul-"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.RandomResizedCrop(160, scale=(0.9, 1.0)),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05),\n",
        "    transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "raw_dataset = []\n",
        "dataset = PathImageFolder(raw_dataset, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=128, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criação do modelo"
      ],
      "metadata": {
        "id": "eA1pO3PeBLCs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqZBm0YJd_WY"
      },
      "outputs": [],
      "source": [
        "class ResidualBottleneckBlock(nn.Module):\n",
        "  expansion = 2\n",
        "  def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "      super().__init__()\n",
        "      #layer 1\n",
        "      self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "      self.bn1 = nn.BatchNorm2d(planes)\n",
        "      #layer 2\n",
        "      self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "      self.bn2 = nn.BatchNorm2d(planes)\n",
        "      #layer 3\n",
        "      self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
        "      self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "\n",
        "      self.relu = nn.ReLU(inplace=True)\n",
        "      self.downsample = downsample\n",
        "\n",
        "  def forward(self, x):\n",
        "      identity = x\n",
        "      out = self.conv1(x)\n",
        "      out = self.bn1(out)\n",
        "      out = self.relu(out)\n",
        "\n",
        "      out = self.conv2(out)\n",
        "      out = self.bn2(out)\n",
        "      out = self.relu(out)\n",
        "\n",
        "      out = self.conv3(out)\n",
        "      out = self.bn3(out)\n",
        "      if self.downsample is not None:\n",
        "          identity = self.downsample(x)\n",
        "      out += identity\n",
        "      out = self.relu(out)\n",
        "      return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSc5DyU8ygNg"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class FaceNetResNet(nn.Module):\n",
        "    def __init__(self, block=ResidualBottleneckBlock, layers=[2,2,2,2], embedding_size=512):\n",
        "        super().__init__()\n",
        "        self.inplanes = 64\n",
        "        #first layer\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        #blocks layers\n",
        "        self.layer1 = self._make_layer(block, 64,  layers[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        #fully connected\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(512 * ResidualBottleneckBlock.expansion, embedding_size)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "      downsample = None\n",
        "      if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "        downsample = nn.Sequential(\n",
        "            nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "            nn.BatchNorm2d(planes * block.expansion),\n",
        "        )\n",
        "      layers = [block(self.inplanes, planes, stride, downsample)]\n",
        "      self.inplanes = planes * block.expansion\n",
        "      for _ in range(1, blocks):\n",
        "          layers.append(block(self.inplanes, planes))\n",
        "      return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return F.normalize(x, p=2, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVjsjIO0s5x5"
      },
      "source": [
        "## Loop de treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOqcDWIxs4ei",
        "outputId": "a3557270-a1e8-4e62-c50f-acb714ed93d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Batch: 0, Loss: 0.8203836679458618\n",
            "Batch: 50, Loss: 0.8307543992996216\n",
            "Epoch 1 complete. Average Loss: 0.8027. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8402153849601746\n",
            "Batch: 50, Loss: 0.7933616042137146\n",
            "Epoch 2 complete. Average Loss: 0.8040. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7919193506240845\n",
            "Batch: 50, Loss: 0.7674819231033325\n",
            "Epoch 3 complete. Average Loss: 0.8015. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8217498064041138\n",
            "Batch: 50, Loss: 0.7852499485015869\n",
            "Epoch 4 complete. Average Loss: 0.8023. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8413223028182983\n",
            "Batch: 50, Loss: 0.7776561379432678\n",
            "Epoch 5 complete. Average Loss: 0.8031. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8121336102485657\n",
            "Batch: 50, Loss: 0.8041819334030151\n",
            "Epoch 6 complete. Average Loss: 0.8015. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8388248085975647\n",
            "Batch: 50, Loss: 0.8160836696624756\n",
            "Epoch 7 complete. Average Loss: 0.8028. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8132558465003967\n",
            "Batch: 50, Loss: 0.8013150691986084\n",
            "Epoch 8 complete. Average Loss: 0.8006. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8023365139961243\n",
            "Batch: 50, Loss: 0.8210452198982239\n",
            "Epoch 9 complete. Average Loss: 0.8018. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7873014807701111\n",
            "Batch: 50, Loss: 0.7770366072654724\n",
            "Epoch 10 complete. Average Loss: 0.7994. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7998208999633789\n",
            "Batch: 50, Loss: 0.8173668384552002\n",
            "Epoch 11 complete. Average Loss: 0.8015. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.807165265083313\n",
            "Batch: 50, Loss: 0.7698168158531189\n",
            "Epoch 12 complete. Average Loss: 0.7991. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8034931421279907\n",
            "Batch: 50, Loss: 0.8046648502349854\n",
            "Epoch 13 complete. Average Loss: 0.8006. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7774351239204407\n",
            "Batch: 50, Loss: 0.8100043535232544\n",
            "Epoch 14 complete. Average Loss: 0.7999. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.833540678024292\n",
            "Batch: 50, Loss: 0.8182548880577087\n",
            "Epoch 15 complete. Average Loss: 0.8006. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8021607398986816\n",
            "Batch: 50, Loss: 0.8178315162658691\n",
            "Epoch 16 complete. Average Loss: 0.8011. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7601215839385986\n",
            "Batch: 50, Loss: 0.8212767839431763\n",
            "Epoch 17 complete. Average Loss: 0.7998. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8023350834846497\n",
            "Batch: 50, Loss: 0.8438500761985779\n",
            "Epoch 18 complete. Average Loss: 0.8009. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.802638053894043\n",
            "Batch: 50, Loss: 0.8243603110313416\n",
            "Epoch 19 complete. Average Loss: 0.7999. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.797829270362854\n",
            "Batch: 50, Loss: 0.7869382500648499\n",
            "Epoch 20 complete. Average Loss: 0.8007. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7996112704277039\n",
            "Batch: 50, Loss: 0.8055658340454102\n",
            "Epoch 21 complete. Average Loss: 0.7991. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7899307012557983\n",
            "Batch: 50, Loss: 0.7857725024223328\n",
            "Epoch 22 complete. Average Loss: 0.7980. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8141215443611145\n",
            "Batch: 50, Loss: 0.8017523288726807\n",
            "Epoch 23 complete. Average Loss: 0.7994. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7770692110061646\n",
            "Batch: 50, Loss: 0.8183902502059937\n",
            "Epoch 24 complete. Average Loss: 0.7994. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8176281452178955\n",
            "Batch: 50, Loss: 0.8133829832077026\n",
            "Epoch 25 complete. Average Loss: 0.7984. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8035696148872375\n",
            "Batch: 50, Loss: 0.8095062971115112\n",
            "Epoch 26 complete. Average Loss: 0.7975. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8408216834068298\n",
            "Batch: 50, Loss: 0.7793570160865784\n",
            "Epoch 27 complete. Average Loss: 0.8001. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8192364573478699\n",
            "Batch: 50, Loss: 0.7745747566223145\n",
            "Epoch 28 complete. Average Loss: 0.7972. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7858099937438965\n",
            "Batch: 50, Loss: 0.8182414174079895\n",
            "Epoch 29 complete. Average Loss: 0.7995. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7656612396240234\n",
            "Batch: 50, Loss: 0.8166183829307556\n",
            "Epoch 30 complete. Average Loss: 0.7953. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8152726292610168\n",
            "Batch: 50, Loss: 0.8009965419769287\n",
            "Epoch 31 complete. Average Loss: 0.7935. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8042823672294617\n",
            "Batch: 50, Loss: 0.7968752980232239\n",
            "Epoch 32 complete. Average Loss: 0.7948. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7776514887809753\n",
            "Batch: 50, Loss: 0.7830840349197388\n",
            "Epoch 33 complete. Average Loss: 0.7937. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7978174686431885\n",
            "Batch: 50, Loss: 0.8039564490318298\n",
            "Epoch 34 complete. Average Loss: 0.7948. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7997824549674988\n",
            "Batch: 50, Loss: 0.7953627109527588\n",
            "Epoch 35 complete. Average Loss: 0.7950. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7697175145149231\n",
            "Batch: 50, Loss: 0.8024829626083374\n",
            "Epoch 36 complete. Average Loss: 0.7933. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8178065419197083\n",
            "Batch: 50, Loss: 0.7727969288825989\n",
            "Epoch 37 complete. Average Loss: 0.7960. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7905395030975342\n",
            "Batch: 50, Loss: 0.795758843421936\n",
            "Epoch 38 complete. Average Loss: 0.7927. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.762005090713501\n",
            "Batch: 50, Loss: 0.790494978427887\n",
            "Epoch 39 complete. Average Loss: 0.7947. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8057835698127747\n",
            "Batch: 50, Loss: 0.7978190779685974\n",
            "Epoch 40 complete. Average Loss: 0.7918. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7794248461723328\n",
            "Batch: 50, Loss: 0.8007614612579346\n",
            "Epoch 41 complete. Average Loss: 0.7952. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7817146182060242\n",
            "Batch: 50, Loss: 0.7943783402442932\n",
            "Epoch 42 complete. Average Loss: 0.7915. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8318555951118469\n",
            "Batch: 50, Loss: 0.7913303971290588\n",
            "Epoch 43 complete. Average Loss: 0.7916. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8150411248207092\n",
            "Batch: 50, Loss: 0.8068444728851318\n",
            "Epoch 44 complete. Average Loss: 0.7916. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7764893174171448\n",
            "Batch: 50, Loss: 0.7747097015380859\n",
            "Epoch 45 complete. Average Loss: 0.7908. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7796118855476379\n",
            "Batch: 50, Loss: 0.8032895922660828\n",
            "Epoch 46 complete. Average Loss: 0.7922. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7420339584350586\n",
            "Batch: 50, Loss: 0.7658677697181702\n",
            "Epoch 47 complete. Average Loss: 0.7888. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7848457098007202\n",
            "Batch: 50, Loss: 0.7999023199081421\n",
            "Epoch 48 complete. Average Loss: 0.7908. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7982966899871826\n",
            "Batch: 50, Loss: 0.7976234555244446\n",
            "Epoch 49 complete. Average Loss: 0.7897. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8068228960037231\n",
            "Batch: 50, Loss: 0.7868539690971375\n",
            "Epoch 50 complete. Average Loss: 0.7908. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8001542687416077\n",
            "Batch: 50, Loss: 0.8028931021690369\n",
            "Epoch 51 complete. Average Loss: 0.7871. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7980745434761047\n",
            "Batch: 50, Loss: 0.7711158990859985\n",
            "Epoch 52 complete. Average Loss: 0.7866. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8140541315078735\n",
            "Batch: 50, Loss: 0.785088062286377\n",
            "Epoch 53 complete. Average Loss: 0.7886. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7845366597175598\n",
            "Batch: 50, Loss: 0.8102869987487793\n",
            "Epoch 54 complete. Average Loss: 0.7857. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7736766934394836\n",
            "Batch: 50, Loss: 0.7825284004211426\n",
            "Epoch 55 complete. Average Loss: 0.7863. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7968454957008362\n",
            "Batch: 50, Loss: 0.7951226830482483\n",
            "Epoch 56 complete. Average Loss: 0.7855. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7786132097244263\n",
            "Batch: 50, Loss: 0.8103998899459839\n",
            "Epoch 57 complete. Average Loss: 0.7860. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8187931776046753\n",
            "Batch: 50, Loss: 0.7858757972717285\n",
            "Epoch 58 complete. Average Loss: 0.7862. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7909886837005615\n",
            "Batch: 50, Loss: 0.7863367795944214\n",
            "Epoch 59 complete. Average Loss: 0.7862. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7812851667404175\n",
            "Batch: 50, Loss: 0.7689803838729858\n",
            "Epoch 60 complete. Average Loss: 0.7864. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7777103185653687\n",
            "Batch: 50, Loss: 0.7851501107215881\n",
            "Epoch 61 complete. Average Loss: 0.7845. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7793887853622437\n",
            "Batch: 50, Loss: 0.769192636013031\n",
            "Epoch 62 complete. Average Loss: 0.7829. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7707595229148865\n",
            "Batch: 50, Loss: 0.7937397956848145\n",
            "Epoch 63 complete. Average Loss: 0.7835. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7489243149757385\n",
            "Batch: 50, Loss: 0.7936558723449707\n",
            "Epoch 64 complete. Average Loss: 0.7822. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.786194384098053\n",
            "Batch: 50, Loss: 0.7929693460464478\n",
            "Epoch 65 complete. Average Loss: 0.7811. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7722115516662598\n",
            "Batch: 50, Loss: 0.7939398884773254\n",
            "Epoch 66 complete. Average Loss: 0.7814. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7692018747329712\n",
            "Batch: 50, Loss: 0.7707956433296204\n",
            "Epoch 67 complete. Average Loss: 0.7808. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8013953566551208\n",
            "Batch: 50, Loss: 0.7864293456077576\n",
            "Epoch 68 complete. Average Loss: 0.7795. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.779352068901062\n",
            "Batch: 50, Loss: 0.7800045013427734\n",
            "Epoch 69 complete. Average Loss: 0.7775. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7624929547309875\n",
            "Batch: 50, Loss: 0.7668250799179077\n",
            "Epoch 70 complete. Average Loss: 0.7798. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7872905135154724\n",
            "Batch: 50, Loss: 0.7951334714889526\n",
            "Epoch 71 complete. Average Loss: 0.7804. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7968588471412659\n",
            "Batch: 50, Loss: 0.7851537466049194\n",
            "Epoch 72 complete. Average Loss: 0.7785. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7508825659751892\n",
            "Batch: 50, Loss: 0.7868388891220093\n",
            "Epoch 73 complete. Average Loss: 0.7786. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7646246552467346\n",
            "Batch: 50, Loss: 0.7981045842170715\n",
            "Epoch 74 complete. Average Loss: 0.7785. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7803806662559509\n",
            "Batch: 50, Loss: 0.8160662651062012\n",
            "Epoch 75 complete. Average Loss: 0.7758. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7907517552375793\n",
            "Batch: 50, Loss: 0.770790159702301\n",
            "Epoch 76 complete. Average Loss: 0.7761. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7938941121101379\n",
            "Batch: 50, Loss: 0.7768716812133789\n",
            "Epoch 77 complete. Average Loss: 0.7759. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7709965705871582\n",
            "Batch: 50, Loss: 0.7338333129882812\n",
            "Epoch 78 complete. Average Loss: 0.7748. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7901362180709839\n",
            "Batch: 50, Loss: 0.7797430157661438\n",
            "Epoch 79 complete. Average Loss: 0.7755. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7885593771934509\n",
            "Batch: 50, Loss: 0.786630392074585\n",
            "Epoch 80 complete. Average Loss: 0.7779. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7624709010124207\n",
            "Batch: 50, Loss: 0.7936355471611023\n",
            "Epoch 81 complete. Average Loss: 0.7736. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7682563066482544\n",
            "Batch: 50, Loss: 0.7770398855209351\n",
            "Epoch 82 complete. Average Loss: 0.7751. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7803633213043213\n",
            "Batch: 50, Loss: 0.7838271856307983\n",
            "Epoch 83 complete. Average Loss: 0.7757. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.8045176267623901\n",
            "Batch: 50, Loss: 0.7801563143730164\n",
            "Epoch 84 complete. Average Loss: 0.7748. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7761087417602539\n",
            "Batch: 50, Loss: 0.808036744594574\n",
            "Epoch 85 complete. Average Loss: 0.7753. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7861447334289551\n",
            "Batch: 50, Loss: 0.7930904030799866\n",
            "Epoch 86 complete. Average Loss: 0.7723. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7678191661834717\n",
            "Batch: 50, Loss: 0.7721489667892456\n",
            "Epoch 87 complete. Average Loss: 0.7744. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7715752124786377\n",
            "Batch: 50, Loss: 0.7789355516433716\n",
            "Epoch 88 complete. Average Loss: 0.7709. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7888242602348328\n",
            "Batch: 50, Loss: 0.7631455659866333\n",
            "Epoch 89 complete. Average Loss: 0.7742. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7924668788909912\n",
            "Batch: 50, Loss: 0.7769061923027039\n",
            "Epoch 90 complete. Average Loss: 0.7716. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7806737422943115\n",
            "Batch: 50, Loss: 0.7562242150306702\n",
            "Epoch 91 complete. Average Loss: 0.7735. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7914184331893921\n",
            "Batch: 50, Loss: 0.7739521861076355\n",
            "Epoch 92 complete. Average Loss: 0.7717. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7551896572113037\n",
            "Batch: 50, Loss: 0.7638977766036987\n",
            "Epoch 93 complete. Average Loss: 0.7705. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7454784512519836\n",
            "Batch: 50, Loss: 0.771430492401123\n",
            "Epoch 94 complete. Average Loss: 0.7683. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7375149726867676\n",
            "Batch: 50, Loss: 0.772226870059967\n",
            "Epoch 95 complete. Average Loss: 0.7645. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7860234975814819\n",
            "Batch: 50, Loss: 0.777740478515625\n",
            "Epoch 96 complete. Average Loss: 0.7707. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7659635543823242\n",
            "Batch: 50, Loss: 0.7756612300872803\n",
            "Epoch 97 complete. Average Loss: 0.7680. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7565669417381287\n",
            "Batch: 50, Loss: 0.7865766882896423\n",
            "Epoch 98 complete. Average Loss: 0.7718. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7805189490318298\n",
            "Batch: 50, Loss: 0.7610137462615967\n",
            "Epoch 99 complete. Average Loss: 0.7664. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7523584365844727\n",
            "Batch: 50, Loss: 0.6885083913803101\n",
            "Epoch 100 complete. Average Loss: 0.7688. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7983747124671936\n",
            "Batch: 50, Loss: 0.7588918209075928\n",
            "Epoch 101 complete. Average Loss: 0.7683. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7474799752235413\n",
            "Batch: 50, Loss: 0.7779683470726013\n",
            "Epoch 102 complete. Average Loss: 0.7617. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7743369936943054\n",
            "Batch: 50, Loss: 0.7801865935325623\n",
            "Epoch 103 complete. Average Loss: 0.7603. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7903487682342529\n",
            "Batch: 50, Loss: 0.7532311677932739\n",
            "Epoch 104 complete. Average Loss: 0.7605. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7487813830375671\n",
            "Batch: 50, Loss: 0.7348046898841858\n",
            "Epoch 105 complete. Average Loss: 0.7581. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7653417587280273\n",
            "Batch: 50, Loss: 0.7804999947547913\n",
            "Epoch 106 complete. Average Loss: 0.7606. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7552834749221802\n",
            "Batch: 50, Loss: 0.7510988712310791\n",
            "Epoch 107 complete. Average Loss: 0.7580. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7495015859603882\n",
            "Batch: 50, Loss: 0.8299800157546997\n",
            "Epoch 108 complete. Average Loss: 0.7595. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7616483569145203\n",
            "Batch: 50, Loss: 0.7380698919296265\n",
            "Epoch 109 complete. Average Loss: 0.7572. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7464333772659302\n",
            "Batch: 50, Loss: 0.7576482892036438\n",
            "Epoch 110 complete. Average Loss: 0.7614. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7524261474609375\n",
            "Batch: 50, Loss: 0.7641880512237549\n",
            "Epoch 111 complete. Average Loss: 0.7539. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7602943181991577\n",
            "Batch: 50, Loss: 0.768950343132019\n",
            "Epoch 112 complete. Average Loss: 0.7548. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7346372008323669\n",
            "Batch: 50, Loss: 0.752055823802948\n",
            "Epoch 113 complete. Average Loss: 0.7537. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7672140598297119\n",
            "Batch: 50, Loss: 0.7406277060508728\n",
            "Epoch 114 complete. Average Loss: 0.7532. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7486506104469299\n",
            "Batch: 50, Loss: 0.7403071522712708\n",
            "Epoch 115 complete. Average Loss: 0.7546. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7240858674049377\n",
            "Batch: 50, Loss: 0.7400298714637756\n",
            "Epoch 116 complete. Average Loss: 0.7571. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7589736580848694\n",
            "Batch: 50, Loss: 0.7331348657608032\n",
            "Epoch 117 complete. Average Loss: 0.7536. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7604052424430847\n",
            "Batch: 50, Loss: 0.7479826807975769\n",
            "Epoch 118 complete. Average Loss: 0.7530. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7681305408477783\n",
            "Batch: 50, Loss: 0.7361416220664978\n",
            "Epoch 119 complete. Average Loss: 0.7528. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7688561677932739\n",
            "Batch: 50, Loss: 0.7618164420127869\n",
            "Epoch 120 complete. Average Loss: 0.7546. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.755581259727478\n",
            "Batch: 50, Loss: 0.7345462441444397\n",
            "Epoch 121 complete. Average Loss: 0.7503. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7569241523742676\n",
            "Batch: 50, Loss: 0.7468528151512146\n",
            "Epoch 122 complete. Average Loss: 0.7527. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7453871965408325\n",
            "Batch: 50, Loss: 0.7471319437026978\n",
            "Epoch 123 complete. Average Loss: 0.7515. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.768702507019043\n",
            "Batch: 50, Loss: 0.7492046356201172\n",
            "Epoch 124 complete. Average Loss: 0.7521. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7743304371833801\n",
            "Batch: 50, Loss: 0.7694224119186401\n",
            "Epoch 125 complete. Average Loss: 0.7496. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.739501953125\n",
            "Batch: 50, Loss: 0.755062997341156\n",
            "Epoch 126 complete. Average Loss: 0.7482. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7265289425849915\n",
            "Batch: 50, Loss: 0.7436655759811401\n",
            "Epoch 127 complete. Average Loss: 0.7507. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7296625375747681\n",
            "Batch: 50, Loss: 0.7447280287742615\n",
            "Epoch 128 complete. Average Loss: 0.7501. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7531424760818481\n",
            "Batch: 50, Loss: 0.7755011320114136\n",
            "Epoch 129 complete. Average Loss: 0.7492. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7564496994018555\n",
            "Batch: 50, Loss: 0.7478380799293518\n",
            "Epoch 130 complete. Average Loss: 0.7500. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7290570735931396\n",
            "Batch: 50, Loss: 0.7602905631065369\n",
            "Epoch 131 complete. Average Loss: 0.7499. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7377933859825134\n",
            "Batch: 50, Loss: 0.7441620230674744\n",
            "Epoch 132 complete. Average Loss: 0.7474. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7560349106788635\n",
            "Batch: 50, Loss: 0.7499388456344604\n",
            "Epoch 133 complete. Average Loss: 0.7519. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7396141290664673\n",
            "Batch: 50, Loss: 0.7288610339164734\n",
            "Epoch 134 complete. Average Loss: 0.7469. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7529832720756531\n",
            "Batch: 50, Loss: 0.7449356913566589\n",
            "Epoch 135 complete. Average Loss: 0.7459. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7283368706703186\n",
            "Batch: 50, Loss: 0.7757347822189331\n",
            "Epoch 136 complete. Average Loss: 0.7458. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7915211319923401\n",
            "Batch: 50, Loss: 0.7538544535636902\n",
            "Epoch 137 complete. Average Loss: 0.7507. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7985168099403381\n",
            "Batch: 50, Loss: 0.7466121315956116\n",
            "Epoch 138 complete. Average Loss: 0.7492. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7489624619483948\n",
            "Batch: 50, Loss: 0.7687110900878906\n",
            "Epoch 139 complete. Average Loss: 0.7477. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7506178021430969\n",
            "Batch: 50, Loss: 0.7514743208885193\n",
            "Epoch 140 complete. Average Loss: 0.7448. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7640270590782166\n",
            "Batch: 50, Loss: 0.723415195941925\n",
            "Epoch 141 complete. Average Loss: 0.7434. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7386860251426697\n",
            "Batch: 50, Loss: 0.71751868724823\n",
            "Epoch 142 complete. Average Loss: 0.7475. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7256114482879639\n",
            "Batch: 50, Loss: 0.7569350600242615\n",
            "Epoch 143 complete. Average Loss: 0.7451. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7419896721839905\n",
            "Batch: 50, Loss: 0.7439925670623779\n",
            "Epoch 144 complete. Average Loss: 0.7440. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7216628193855286\n",
            "Batch: 50, Loss: 0.7396163940429688\n",
            "Epoch 145 complete. Average Loss: 0.7436. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.78272545337677\n",
            "Batch: 50, Loss: 0.7620015144348145\n",
            "Epoch 146 complete. Average Loss: 0.7447. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7108698487281799\n",
            "Batch: 50, Loss: 0.7087854146957397\n",
            "Epoch 147 complete. Average Loss: 0.7479. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7468432188034058\n",
            "Batch: 50, Loss: 0.7343764305114746\n",
            "Epoch 148 complete. Average Loss: 0.7400. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7414069175720215\n",
            "Batch: 50, Loss: 0.745535671710968\n",
            "Epoch 149 complete. Average Loss: 0.7412. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7543525695800781\n",
            "Batch: 50, Loss: 0.7315972447395325\n",
            "Epoch 150 complete. Average Loss: 0.7405. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7244627475738525\n",
            "Batch: 50, Loss: 0.7479134202003479\n",
            "Epoch 151 complete. Average Loss: 0.7373. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7304214239120483\n",
            "Batch: 50, Loss: 0.7557646632194519\n",
            "Epoch 152 complete. Average Loss: 0.7406. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7376752495765686\n",
            "Batch: 50, Loss: 0.7441903352737427\n",
            "Epoch 153 complete. Average Loss: 0.7350. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7262383699417114\n",
            "Batch: 50, Loss: 0.7528015971183777\n",
            "Epoch 154 complete. Average Loss: 0.7386. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7098984122276306\n",
            "Batch: 50, Loss: 0.7503082156181335\n",
            "Epoch 155 complete. Average Loss: 0.7377. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7314481139183044\n",
            "Batch: 50, Loss: 0.7080838084220886\n",
            "Epoch 156 complete. Average Loss: 0.7313. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.764895498752594\n",
            "Batch: 50, Loss: 0.7450200319290161\n",
            "Epoch 157 complete. Average Loss: 0.7409. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7388248443603516\n",
            "Batch: 50, Loss: 0.7467911839485168\n",
            "Epoch 158 complete. Average Loss: 0.7349. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7280012369155884\n",
            "Batch: 50, Loss: 0.713334858417511\n",
            "Epoch 159 complete. Average Loss: 0.7340. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7313388586044312\n",
            "Batch: 50, Loss: 0.7251699566841125\n",
            "Epoch 160 complete. Average Loss: 0.7343. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7515952587127686\n",
            "Batch: 50, Loss: 0.7545844316482544\n",
            "Epoch 161 complete. Average Loss: 0.7375. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7260203957557678\n",
            "Batch: 50, Loss: 0.7244008183479309\n",
            "Epoch 162 complete. Average Loss: 0.7351. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7374238967895508\n",
            "Batch: 50, Loss: 0.7167999744415283\n",
            "Epoch 163 complete. Average Loss: 0.7308. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7359551191329956\n",
            "Batch: 50, Loss: 0.7268474102020264\n",
            "Epoch 164 complete. Average Loss: 0.7308. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7509657740592957\n",
            "Batch: 50, Loss: 0.7420755624771118\n",
            "Epoch 165 complete. Average Loss: 0.7307. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7105022072792053\n",
            "Batch: 50, Loss: 0.713624894618988\n",
            "Epoch 166 complete. Average Loss: 0.7302. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.709673285484314\n",
            "Batch: 50, Loss: 0.7319256067276001\n",
            "Epoch 167 complete. Average Loss: 0.7244. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7123789191246033\n",
            "Batch: 50, Loss: 0.7207368612289429\n",
            "Epoch 168 complete. Average Loss: 0.7252. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7443384528160095\n",
            "Batch: 50, Loss: 0.724912166595459\n",
            "Epoch 169 complete. Average Loss: 0.7279. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7487348318099976\n",
            "Batch: 50, Loss: 0.729402482509613\n",
            "Epoch 170 complete. Average Loss: 0.7297. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.6988509297370911\n",
            "Batch: 50, Loss: 0.7378149032592773\n",
            "Epoch 171 complete. Average Loss: 0.7279. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.699752151966095\n",
            "Batch: 50, Loss: 0.6981557011604309\n",
            "Epoch 172 complete. Average Loss: 0.7264. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7198916077613831\n",
            "Batch: 50, Loss: 0.7595427632331848\n",
            "Epoch 173 complete. Average Loss: 0.7279. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7549030184745789\n",
            "Batch: 50, Loss: 0.7139489054679871\n",
            "Epoch 174 complete. Average Loss: 0.7297. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7158332467079163\n",
            "Batch: 50, Loss: 0.7666189074516296\n",
            "Epoch 175 complete. Average Loss: 0.7291. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.709054172039032\n",
            "Batch: 50, Loss: 0.7003183960914612\n",
            "Epoch 176 complete. Average Loss: 0.7265. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7166586518287659\n",
            "Batch: 50, Loss: 0.7494531273841858\n",
            "Epoch 177 complete. Average Loss: 0.7301. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7138574123382568\n",
            "Batch: 50, Loss: 0.7416470646858215\n",
            "Epoch 178 complete. Average Loss: 0.7289. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7364232540130615\n",
            "Batch: 50, Loss: 0.7427012324333191\n",
            "Epoch 179 complete. Average Loss: 0.7277. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7044046521186829\n",
            "Batch: 50, Loss: 0.7146015763282776\n",
            "Epoch 180 complete. Average Loss: 0.7256. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7122809886932373\n",
            "Batch: 50, Loss: 0.7497845888137817\n",
            "Epoch 181 complete. Average Loss: 0.7283. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7327022552490234\n",
            "Batch: 50, Loss: 0.7421619296073914\n",
            "Epoch 182 complete. Average Loss: 0.7263. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7209169268608093\n",
            "Batch: 50, Loss: 0.7382736206054688\n",
            "Epoch 183 complete. Average Loss: 0.7277. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7567502856254578\n",
            "Batch: 50, Loss: 0.7474296689033508\n",
            "Epoch 184 complete. Average Loss: 0.7262. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7307179570198059\n",
            "Batch: 50, Loss: 0.7389110326766968\n",
            "Epoch 185 complete. Average Loss: 0.7253. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7639861106872559\n",
            "Batch: 50, Loss: 0.6936417818069458\n",
            "Epoch 186 complete. Average Loss: 0.7218. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7185259461402893\n",
            "Batch: 50, Loss: 0.7221161127090454\n",
            "Epoch 187 complete. Average Loss: 0.7224. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.734056830406189\n",
            "Batch: 50, Loss: 0.764553427696228\n",
            "Epoch 188 complete. Average Loss: 0.7192. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.732340931892395\n",
            "Batch: 50, Loss: 0.688369631767273\n",
            "Epoch 189 complete. Average Loss: 0.7228. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7297646999359131\n",
            "Batch: 50, Loss: 0.7370361089706421\n",
            "Epoch 190 complete. Average Loss: 0.7246. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7154802680015564\n",
            "Batch: 50, Loss: 0.7199510931968689\n",
            "Epoch 191 complete. Average Loss: 0.7253. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7628971934318542\n",
            "Batch: 50, Loss: 0.7536692023277283\n",
            "Epoch 192 complete. Average Loss: 0.7262. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.6940968632698059\n",
            "Batch: 50, Loss: 0.7199110388755798\n",
            "Epoch 193 complete. Average Loss: 0.7251. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7226190567016602\n",
            "Batch: 50, Loss: 0.7057301998138428\n",
            "Epoch 194 complete. Average Loss: 0.7251. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7202268838882446\n",
            "Batch: 50, Loss: 0.7424544095993042\n",
            "Epoch 195 complete. Average Loss: 0.7279. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.6880221366882324\n",
            "Batch: 50, Loss: 0.7213115692138672\n",
            "Epoch 196 complete. Average Loss: 0.7219. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7309259176254272\n",
            "Batch: 50, Loss: 0.753373920917511\n",
            "Epoch 197 complete. Average Loss: 0.7226. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7553514838218689\n",
            "Batch: 50, Loss: 0.7053309679031372\n",
            "Epoch 198 complete. Average Loss: 0.7237. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7263957262039185\n",
            "Batch: 50, Loss: 0.7430214881896973\n",
            "Epoch 199 complete. Average Loss: 0.7241. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.6941753029823303\n",
            "Batch: 50, Loss: 0.7380673885345459\n",
            "Epoch 200 complete. Average Loss: 0.7232. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7615649104118347\n",
            "Batch: 50, Loss: 0.7159780859947205\n",
            "Epoch 201 complete. Average Loss: 0.7226. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7491037249565125\n",
            "Batch: 50, Loss: 0.7416456341743469\n",
            "Epoch 202 complete. Average Loss: 0.7287. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7367315292358398\n",
            "Batch: 50, Loss: 0.7058106660842896\n",
            "Epoch 203 complete. Average Loss: 0.7230. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7371081709861755\n",
            "Batch: 50, Loss: 0.7038301229476929\n",
            "Epoch 204 complete. Average Loss: 0.7244. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7704982757568359\n",
            "Batch: 50, Loss: 0.7401149272918701\n",
            "Epoch 205 complete. Average Loss: 0.7239. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7141275405883789\n",
            "Batch: 50, Loss: 0.7435703873634338\n",
            "Epoch 206 complete. Average Loss: 0.7227. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7169424891471863\n",
            "Batch: 50, Loss: 0.7095806002616882\n",
            "Epoch 207 complete. Average Loss: 0.7216. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7249999642372131\n",
            "Batch: 50, Loss: 0.6805541515350342\n",
            "Epoch 208 complete. Average Loss: 0.7237. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7477297782897949\n",
            "Batch: 50, Loss: 0.750809371471405\n",
            "Epoch 209 complete. Average Loss: 0.7277. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.696234405040741\n",
            "Batch: 50, Loss: 0.7057398557662964\n",
            "Epoch 210 complete. Average Loss: 0.7247. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7198655009269714\n",
            "Batch: 50, Loss: 0.7324978709220886\n",
            "Epoch 211 complete. Average Loss: 0.7228. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.6901665925979614\n",
            "Batch: 50, Loss: 0.7224395871162415\n",
            "Epoch 212 complete. Average Loss: 0.7257. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.6858898401260376\n",
            "Batch: 50, Loss: 0.7256693840026855\n",
            "Epoch 213 complete. Average Loss: 0.7204. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7132886052131653\n",
            "Batch: 50, Loss: 0.7129074335098267\n",
            "Epoch 214 complete. Average Loss: 0.7194. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7159169316291809\n",
            "Batch: 50, Loss: 0.7161693572998047\n",
            "Epoch 215 complete. Average Loss: 0.7272. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7118368148803711\n",
            "Batch: 50, Loss: 0.735018253326416\n",
            "Epoch 216 complete. Average Loss: 0.7248. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7489073276519775\n",
            "Batch: 50, Loss: 0.713752269744873\n",
            "Epoch 217 complete. Average Loss: 0.7232. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.694862425327301\n",
            "Batch: 50, Loss: 0.6992982625961304\n",
            "Epoch 218 complete. Average Loss: 0.7243. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7214065194129944\n",
            "Batch: 50, Loss: 0.7296739816665649\n",
            "Epoch 219 complete. Average Loss: 0.7240. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7428426742553711\n",
            "Batch: 50, Loss: 0.7307578921318054\n",
            "Epoch 220 complete. Average Loss: 0.7237. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7242603302001953\n",
            "Batch: 50, Loss: 0.7042509913444519\n",
            "Epoch 221 complete. Average Loss: 0.7252. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7017872929573059\n",
            "Batch: 50, Loss: 0.7459076642990112\n",
            "Epoch 222 complete. Average Loss: 0.7252. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7452239394187927\n",
            "Batch: 50, Loss: 0.7214281558990479\n",
            "Epoch 223 complete. Average Loss: 0.7250. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7323855757713318\n",
            "Batch: 50, Loss: 0.7338970303535461\n",
            "Epoch 224 complete. Average Loss: 0.7242. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.724988579750061\n",
            "Batch: 50, Loss: 0.7217836976051331\n",
            "Epoch 225 complete. Average Loss: 0.7220. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7384114265441895\n",
            "Batch: 50, Loss: 0.6978608965873718\n",
            "Epoch 226 complete. Average Loss: 0.7220. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.729478657245636\n",
            "Batch: 50, Loss: 0.7177006006240845\n",
            "Epoch 227 complete. Average Loss: 0.7230. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7314133048057556\n",
            "Batch: 50, Loss: 0.7575917840003967\n",
            "Epoch 228 complete. Average Loss: 0.7260. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7103258967399597\n",
            "Batch: 50, Loss: 0.7283115983009338\n",
            "Epoch 229 complete. Average Loss: 0.7250. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7241665124893188\n",
            "Batch: 50, Loss: 0.6764732003211975\n",
            "Epoch 230 complete. Average Loss: 0.7236. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.6934772729873657\n",
            "Batch: 50, Loss: 0.7216590046882629\n",
            "Epoch 231 complete. Average Loss: 0.7221. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7310803532600403\n",
            "Batch: 50, Loss: 0.7317387461662292\n",
            "Epoch 232 complete. Average Loss: 0.7249. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7222412824630737\n",
            "Batch: 50, Loss: 0.7021322250366211\n",
            "Epoch 233 complete. Average Loss: 0.7264. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.6813780069351196\n",
            "Batch: 50, Loss: 0.7371503710746765\n",
            "Epoch 234 complete. Average Loss: 0.7213. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.740279495716095\n",
            "Batch: 50, Loss: 0.743704617023468\n",
            "Epoch 235 complete. Average Loss: 0.7201. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7487387657165527\n",
            "Batch: 50, Loss: 0.7050866484642029\n",
            "Epoch 236 complete. Average Loss: 0.7260. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7325319647789001\n",
            "Batch: 50, Loss: 0.7079089879989624\n",
            "Epoch 237 complete. Average Loss: 0.7232. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7060205936431885\n",
            "Batch: 50, Loss: 0.7143331170082092\n",
            "Epoch 238 complete. Average Loss: 0.7237. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7177850604057312\n",
            "Batch: 50, Loss: 0.6966776251792908\n",
            "Epoch 239 complete. Average Loss: 0.7242. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7497088313102722\n",
            "Batch: 50, Loss: 0.7419180274009705\n",
            "Epoch 240 complete. Average Loss: 0.7258. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7482650279998779\n",
            "Batch: 50, Loss: 0.7157114148139954\n",
            "Epoch 241 complete. Average Loss: 0.7221. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7243690490722656\n",
            "Batch: 50, Loss: 0.7169652581214905\n",
            "Epoch 242 complete. Average Loss: 0.7270. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7112891674041748\n",
            "Batch: 50, Loss: 0.7271438241004944\n",
            "Epoch 243 complete. Average Loss: 0.7233. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7288051843643188\n",
            "Batch: 50, Loss: 0.7023555040359497\n",
            "Epoch 244 complete. Average Loss: 0.7219. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7161549925804138\n",
            "Batch: 50, Loss: 0.731232225894928\n",
            "Epoch 245 complete. Average Loss: 0.7261. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7038227319717407\n",
            "Batch: 50, Loss: 0.7354897260665894\n",
            "Epoch 246 complete. Average Loss: 0.7234. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7366641759872437\n",
            "Batch: 50, Loss: 0.7136768698692322\n",
            "Epoch 247 complete. Average Loss: 0.7258. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7317068576812744\n",
            "Batch: 50, Loss: 0.727618932723999\n",
            "Epoch 248 complete. Average Loss: 0.7235. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7168683409690857\n",
            "Batch: 50, Loss: 0.7309924960136414\n",
            "Epoch 249 complete. Average Loss: 0.7217. Skipped batches: 0/94\n",
            "Batch: 0, Loss: 0.7075016498565674\n",
            "Batch: 50, Loss: 0.7076197862625122\n",
            "Epoch 250 complete. Average Loss: 0.7192. Skipped batches: 0/94\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = FaceNetResNet().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
        "loss_fn = losses.TripletMarginLoss(margin=0.8)\n",
        "miner = miners.MultiSimilarityMiner(epsilon=0.1)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "epochs = 250\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  skipped = 0\n",
        "  total_loss = 0\n",
        "  batch_count = 0\n",
        "\n",
        "  for batch_idx, batch in enumerate(dataloader):\n",
        "    imgs, paths, labels = batch\n",
        "    imgs = imgs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    embeddings = model(imgs)\n",
        "    hard_triplets = miner(embeddings, labels)\n",
        "\n",
        "    if len(hard_triplets) == 0:\n",
        "      skipped+= 1\n",
        "      continue\n",
        "\n",
        "    loss = loss_fn(embeddings, labels, hard_triplets)\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    batch_count += 1\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_idx % 50 == 0:\n",
        "      print(f\"Batch: {batch_idx}, Loss: {loss.item()}\")\n",
        "\n",
        "  avg_loss = total_loss / batch_count if batch_count > 0 else 0\n",
        "  scheduler.step(avg_loss)\n",
        "  print(f\"Epoch {epoch+1} complete. Average Loss: {avg_loss:.4f}. Skipped batches: {skipped}/{len(dataloader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "usa o modelo em todas as imagens e as salva na db"
      ],
      "metadata": {
        "id": "JmKh1fxgBl04"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_bkKFrS8vC_"
      },
      "outputs": [],
      "source": [
        "all_embeddings = []\n",
        "all_labels = []\n",
        "all_paths = []\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, paths, labels in dataloader:\n",
        "      imgs = imgs.to(device)\n",
        "      emb = model(imgs).cpu().numpy()\n",
        "      all_embeddings.extend(emb.tolist())\n",
        "      all_paths.extend(paths)\n",
        "      all_labels.extend(labels.tolist())\n",
        "\n",
        "collection = client.create_collection(\n",
        "    name=\"faces\",\n",
        "    embedding_function=None,\n",
        "    get_or_create=True\n",
        ")\n",
        "\n",
        "chromas_batch_size = 5000\n",
        "num_embeddings = len(all_embeddings)\n",
        "\n",
        "for i in range(0, num_embeddings, chromas_batch_size):\n",
        "    batch_embeddings = all_embeddings[i:i+chromas_batch_size]\n",
        "    batch_ids = [f\"img_{j}\" for j in range(i, i+len(batch_embeddings))]\n",
        "    batch_metadatas = [{\"label\": label} for label in all_labels[i:i+chromas_batch_size]]\n",
        "    batch_documents = [path for path in all_paths[i:i+chromas_batch_size]]\n",
        "\n",
        "    collection.add(\n",
        "        ids=batch_ids,\n",
        "        embeddings=batch_embeddings,\n",
        "        metadatas=batch_metadatas,\n",
        "        documents=batch_documents\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}